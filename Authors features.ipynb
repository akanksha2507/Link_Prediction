{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random, csv\n",
    "import numpy as np\n",
    "import cPickle\n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn import preprocessing\n",
    "import nltk\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading test and train data, and node info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#nltk.download('punkt') # for tokenization\n",
    "#nltk.download('stopwords')\n",
    "stpwds = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "stemmer_new = nltk.stem.SnowballStemmer(\"english\")\n",
    "with open(\"testing_set.txt\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    testing_set  = list(reader)\n",
    "\n",
    "testing_set = [element[0].split(\" \") for element in testing_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"training_set.txt\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    training_set  = list(reader)\n",
    "\n",
    "training_set = [element[0].split(\" \") for element in training_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"node_information.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    node_info  = list(reader)\n",
    "\n",
    "IDs = [element[0] for element in node_info]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 training examples processsed\n",
      "20001 training examples processsed\n",
      "40001 training examples processsed\n",
      "60001 training examples processsed\n",
      "80001 training examples processsed\n",
      "100001 training examples processsed\n",
      "120001 training examples processsed\n",
      "140001 training examples processsed\n",
      "160001 training examples processsed\n",
      "180001 training examples processsed\n",
      "200001 training examples processsed\n",
      "220001 training examples processsed\n",
      "240001 training examples processsed\n",
      "260001 training examples processsed\n",
      "280001 training examples processsed\n",
      "300001 training examples processsed\n",
      "320001 training examples processsed\n",
      "340001 training examples processsed\n",
      "360001 training examples processsed\n",
      "380001 training examples processsed\n",
      "400001 training examples processsed\n",
      "420001 training examples processsed\n",
      "440001 training examples processsed\n",
      "460001 training examples processsed\n",
      "480001 training examples processsed\n",
      "500001 training examples processsed\n",
      "520001 training examples processsed\n",
      "540001 training examples processsed\n",
      "560001 training examples processsed\n",
      "580001 training examples processsed\n",
      "600001 training examples processsed\n"
     ]
    }
   ],
   "source": [
    "file=open(\"features_train_unscaled.csv\",'r')\n",
    "lines=file.readlines()\n",
    "feat = []\n",
    "counter = 0\n",
    "collab = {}\n",
    "cite = {}\n",
    "from collections import defaultdict\n",
    "for line in lines[1:]:\n",
    "    line=line.strip().split(\",\")  \n",
    "    feat.append(line)\n",
    "    source = line[0]\n",
    "    target = line[1]\n",
    "    \n",
    "    source_info = [element for element in node_info if element[0]==source][0]\n",
    "    target_info = [element for element in node_info if element[0]==target][0]\n",
    "    \n",
    "    #source_title = source_info[4].lower().split(\".\")\n",
    "    authors_A = source_info[3].split(',')\n",
    "    authors_B = target_info[3].split(',')\n",
    "    \n",
    "    # citations\n",
    "    for auth_a in authors_A:\n",
    "        a = auth_a.strip()\n",
    "        for auth_b in authors_B:\n",
    "            b = auth_b.strip()\n",
    "            if a == '' or b == '':\n",
    "                continue\n",
    "            if a not in cite:\n",
    "                cite[a] = defaultdict(int)\n",
    "            cite[a][b] += 1\n",
    "    \n",
    "    # collaborations\n",
    "    for auth_1 in authors_A:\n",
    "        a = auth_1.strip()\n",
    "        for auth_2 in authors_A:\n",
    "            b = auth_2.strip()\n",
    "            if a == '' or b == '':\n",
    "                continue\n",
    "            if a not in collab:\n",
    "                collab[a] = defaultdict(int)\n",
    "            collab[a][b] += 1\n",
    "            \n",
    "    for auth_1 in authors_B:\n",
    "        a = auth_1.strip()\n",
    "        for auth_2 in authors_B:\n",
    "            b = auth_2.strip()\n",
    "            if a == '' or b == '':\n",
    "                continue\n",
    "            if a not in collab:\n",
    "                collab[a] = defaultdict(int)\n",
    "            collab[a][b] += 1\n",
    "    \n",
    "    counter += 1\n",
    "    if counter % 20000 == True:\n",
    "        print counter, \"training examples processsed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO WRITE DICTIONARIES TO A FILE\n",
    "\n",
    "#import json\n",
    "#with open('cite.txt', 'w+') as f:\n",
    "#    json.dump(cite, f)\n",
    "#import json\n",
    "#with open('collab.txt', 'w+') as f:\n",
    "#    json.dump(collab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-fc0e22781ee2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'collab.txt'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mcollab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cite.txt'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mcite\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'json' is not defined"
     ]
    }
   ],
   "source": [
    "# TO LOAD DICTIONARIES FROM FILE\n",
    "\n",
    "#with open('collab.txt') as f:\n",
    "#    collab = json.load(f)\n",
    "#with open('cite.txt') as f:\n",
    "#    cite = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating graph (nodes and edges from training data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "edges = [(element[0],element[1]) for element in training_set if element[2]==\"1\"]\n",
    "nodes = [element[0] for element in training_set] + [element[1] for element in training_set]\n",
    "\n",
    "# this is the actual directed graph\n",
    "G = nx.DiGraph()\n",
    "G.add_nodes_from(nodes)\n",
    "G.add_edges_from(edges)\n",
    "\n",
    "# undirected version of the graph (used for jaccard similarity)\n",
    "G_undirected = nx.Graph()\n",
    "G_undirected.add_nodes_from(nodes)\n",
    "G_undirected.add_edges_from(edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating features for the entire data to store in csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 training examples processsed\n",
      "20001 training examples processsed\n",
      "40001 training examples processsed\n",
      "60001 training examples processsed\n",
      "80001 training examples processsed\n",
      "100001 training examples processsed\n",
      "120001 training examples processsed\n",
      "140001 training examples processsed\n",
      "160001 training examples processsed\n",
      "180001 training examples processsed\n",
      "200001 training examples processsed\n",
      "220001 training examples processsed\n",
      "240001 training examples processsed\n",
      "260001 training examples processsed\n",
      "280001 training examples processsed\n",
      "300001 training examples processsed\n",
      "320001 training examples processsed\n",
      "340001 training examples processsed\n",
      "360001 training examples processsed\n",
      "380001 training examples processsed\n",
      "400001 training examples processsed\n",
      "420001 training examples processsed\n",
      "440001 training examples processsed\n",
      "460001 training examples processsed\n",
      "480001 training examples processsed\n",
      "500001 training examples processsed\n",
      "520001 training examples processsed\n",
      "540001 training examples processsed\n",
      "560001 training examples processsed\n",
      "580001 training examples processsed\n",
      "600001 training examples processsed\n"
     ]
    }
   ],
   "source": [
    "### creating features and store in a csv file\n",
    "\n",
    "max_collab_A_B = [] # max collaboration between authors of A and B\n",
    "mean_collab_A_B = []\n",
    "\n",
    "max_cite_A_B = [] # max citation by authors of A to B\n",
    "mean_cite_A_B = []\n",
    "\n",
    "#avg_cite_A_1 = [] # average citations authors of source have received\n",
    "avg_cite_A_2 = [] # average citations authors of source have made\n",
    "\n",
    "#avg_cite_B_1 = [] # average citations authors of target have received\n",
    "avg_cite_B_2 = [] # average citations authors of target have made\n",
    "\n",
    "indegree = []\n",
    "counter = 0\n",
    "for i in xrange(len(training_set)):\n",
    "    source = training_set[i][0]\n",
    "    target = training_set[i][1]\n",
    "    \n",
    "    source_info = [element for element in node_info if element[0]==source][0]\n",
    "    target_info = [element for element in node_info if element[0]==target][0]\n",
    "    \n",
    "    authors_A = source_info[3].split(',')\n",
    "    authors_B = target_info[3].split(',')\n",
    "    \n",
    "    max_collab = []\n",
    "    max_cite = []\n",
    "    for auth_a in authors_A:\n",
    "        a = auth_a.strip()\n",
    "        for auth_b in authors_B:\n",
    "            b = auth_b.strip()\n",
    "            if a == '' or b == '':\n",
    "                continue\n",
    "            max_collab.append(collab[a][b])\n",
    "            max_cite.append(cite[a][b])\n",
    "            \n",
    "    max_collab_A_B.append(0 if not len(max_collab) else max(max_collab))\n",
    "    mean_collab_A_B.append(0 if not len(max_collab) else sum(max_collab)/len(max_collab))\n",
    "    \n",
    "    max_cite_A_B.append(0 if not len(max_cite) else max(max_cite) )\n",
    "    mean_cite_A_B.append(0 if not len(max_cite) else sum(max_cite)/len(max_cite))\n",
    "    \n",
    "    avg_cite = []\n",
    "    for auth_a in authors_A:\n",
    "        a = auth_a.strip()\n",
    "        if a == '' or not a in cite:\n",
    "            continue\n",
    "        avg_cite.append(sum(cite[a].values()))\n",
    "    avg_cite_A_2.append(0 if not len(avg_cite) else sum(avg_cite)/len(avg_cite))\n",
    "    \n",
    "    avg_cite = []\n",
    "    for auth_a in authors_B:\n",
    "        a = auth_a.strip()\n",
    "        if a == '' or not a in cite:\n",
    "            continue\n",
    "        avg_cite.append(sum(cite[a].values()))\n",
    "    avg_cite_B_2.append(0 if not len(avg_cite) else sum(avg_cite)/len(avg_cite))\n",
    "    \n",
    "    indegree.append(G.in_degree(target)/len(authors_B))\n",
    "    counter += 1\n",
    "    if counter % 20000 == True:\n",
    "        print counter, \"training examples processsed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-65f1677c86f9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenfromtxt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'features_train_unscaled.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "train_features = np.genfromtxt('features_train_unscaled.csv', delimiter=',')[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(615512L, 7L) (7L,) [  0   0   0   0   0 244   2]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([max_collab_A_B, mean_collab_A_B, max_cite_A_B, mean_cite_A_B, avg_cite_A_2, avg_cite_B_2, indegree]).T\n",
    "print X.shape, X[0].shape, X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_features = np.array([max_collab_A_B, mean_collab_A_B, max_cite_A_B, mean_cite_A_B, avg_cite_A_2, avg_cite_B_2, indegree]).T\n",
    "train_features = np.genfromtxt('features_train_unscaled3.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # convert list of lists into array\n",
    "# # documents as rows, unique words as columns (i.e., example as rows, features as columns)\n",
    "# training_features = np.array([overlap_title, temp_diff, comm_auth, jaccard_sim]).T\n",
    "\n",
    "# # scale\n",
    "# training_features = preprocessing.scale(training_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(615513L, 10L) (615512L, 7L) [ nan  nan  nan  nan  nan  nan  nan  nan  nan  nan]\n"
     ]
    }
   ],
   "source": [
    "print train_features.shape, new_features.shape, train_features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(615512L, 17L)\n"
     ]
    }
   ],
   "source": [
    "training_features = np.concatenate((train_features[1:], new_features), axis=1)\n",
    "print training_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-c78e7e5cffde>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mprint\u001b[0m \u001b[0mtraining_features\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'training_features' is not defined"
     ]
    }
   ],
   "source": [
    "print training_features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write features to .csv file\n",
    "X_train = np.array(training_set[:]).T\n",
    "X_features = training_features[:].T\n",
    "features = zip(X_features[0], X_features[1], X_features[2], X_features[3], X_features[4], \\\n",
    "              X_features[5], X_features[6], X_features[7], X_features[8], X_features[9], X_features[10], X_features[11], \\\n",
    "              X_features[12], X_features[13], X_features[14], X_features[15], X_features[16])\n",
    "with open(\"features_train_unscaled_authors.csv\",\"wb\") as feat:\n",
    "    csv_out = csv.writer(feat)\n",
    "    csv_out.writerow(('source', 'target', 'label', 'overlap_title', 'temp_diff', 'common_author', 'overlap_summary',\\\n",
    "                      'jaccard_sim', 'adamic', 'tfidf_cosine_sim', 'max_collab', 'mean_collab', 'max_cite', 'mean_cite', \\\n",
    "                      'avg_cite_A', 'avg_cite_B', 'indegree_target'))\n",
    "    for row in features:\n",
    "        csv_out.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['9510123.0', '9502114.0', '1.0', '2.0', '0.0', '0.0', '2.0',\n",
       "       '0.0588235294118', '0.51389834237', '0.126084128029', '8'], \n",
       "      dtype='|S32')"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_features[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing features for test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'dict'>\n"
     ]
    }
   ],
   "source": [
    "#tfidf_dict_test = cPickle.load(open('tfidf_features_test.p', 'rb'))\n",
    "#print type(tfidf_dict_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 testing examples processsed\n",
      "10001 testing examples processsed\n",
      "20001 testing examples processsed\n",
      "30001 testing examples processsed\n"
     ]
    }
   ],
   "source": [
    "### creating features and store in a csv file\n",
    "max_collab_A_B = [] # max collaboration between authors of A and B\n",
    "mean_collab_A_B = []\n",
    "\n",
    "max_cite_A_B = [] # max citation by authors of A to B\n",
    "mean_cite_A_B = []\n",
    "\n",
    "#avg_cite_A_1 = [] # average citations authors of source have received\n",
    "avg_cite_A_2 = [] # average citations authors of source have made\n",
    "\n",
    "#avg_cite_B_1 = [] # average citations authors of target have received\n",
    "avg_cite_B_2 = [] # average citations authors of target have made\n",
    "\n",
    "indegree = []\n",
    "\n",
    "counter = 0\n",
    "for i in xrange(len(testing_set)):\n",
    "    source = testing_set[i][0]\n",
    "    target = testing_set[i][1]\n",
    "    \n",
    "    authors_A = source_info[3].split(',')\n",
    "    authors_B = target_info[3].split(',')\n",
    "    \n",
    "    max_collab = []\n",
    "    max_cite = []\n",
    "    for auth_a in authors_A:\n",
    "        a = auth_a.strip()\n",
    "        for auth_b in authors_B:\n",
    "            b = auth_b.strip()\n",
    "            if a == '' or b == '':\n",
    "                continue\n",
    "            max_collab.append(collab[a][b])\n",
    "            max_cite.append(cite[a][b])\n",
    "            \n",
    "    max_collab_A_B.append(0 if not len(max_collab) else max(max_collab))\n",
    "    mean_collab_A_B.append(0 if not len(max_collab) else sum(max_collab)/len(max_collab))\n",
    "    \n",
    "    max_cite_A_B.append(0 if not len(max_cite) else max(max_cite) )\n",
    "    mean_cite_A_B.append(0 if not len(max_cite) else sum(max_cite)/len(max_cite))\n",
    "    \n",
    "    avg_cite = []\n",
    "    for auth_a in authors_A:\n",
    "        a = auth_a.strip()\n",
    "        if a == '' or not a in cite:\n",
    "            continue\n",
    "        avg_cite.append(sum(cite[a].values()))\n",
    "    avg_cite_A_2.append(0 if not len(avg_cite) else sum(avg_cite)/len(avg_cite))\n",
    "    \n",
    "    avg_cite = []\n",
    "    for auth_a in authors_B:\n",
    "        a = auth_a.strip()\n",
    "        if a == '' or not a in cite:\n",
    "            continue\n",
    "        avg_cite.append(sum(cite[a].values()))\n",
    "    avg_cite_B_2.append(0 if not len(avg_cite) else sum(avg_cite)/len(avg_cite))\n",
    "    \n",
    "    indegree.append(G.in_degree(target)/len(authors_B))\n",
    "    counter += 1\n",
    "    if counter % 10000 == True:\n",
    "        print counter, \"testing examples processsed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_features = np.array([max_collab_A_B, mean_collab_A_B, max_cite_A_B, mean_cite_A_B, avg_cite_A_2, avg_cite_B_2, indegree]).T\n",
    "test_features = np.genfromtxt('features_test_unscaled.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32649L, 9L) (32648L, 7L)\n"
     ]
    }
   ],
   "source": [
    "print test_features.shape, new_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32648L, 16L)\n"
     ]
    }
   ],
   "source": [
    "testing_features = np.concatenate((test_features[1:], new_features), axis=1)\n",
    "print testing_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  9.80707600e+06   9.80713900e+06   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   9.44587143e-02   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   2.51100000e+03   0.00000000e+00   3.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "print testing_features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write features to .csv file\n",
    "X_test = np.array(testing_set[:]).T\n",
    "X_test_features = testing_features.T\n",
    "features = zip(X_test_features[0], X_test_features[1], X_test_features[2], X_test_features[3], X_test_features[4], \\\n",
    "              X_test_features[5], X_test_features[6], X_test_features[7], X_test_features[8], X_test_features[9], \\\n",
    "              X_test_features[10], X_test_features[11], X_test_features[12], X_test_features[13], X_test_features[14],\n",
    "              X_test_features[15])\n",
    "with open(\"features_test_unscaled_authors.csv\",\"wb\") as feat:\n",
    "    csv_out = csv.writer(feat)\n",
    "    csv_out.writerow(('source', 'target', 'overlap_title', 'temp_diff', 'common_author', 'overlap_summary', 'jaccard_sim'\\\n",
    "                     ,'adamic', 'tfidf_cosine_sim', 'max_collab', 'mean_collab', 'max_cite', 'mean_cite', \\\n",
    "                      'avg_cite_A', 'avg_cite_B', 'indegree_target'))\n",
    "    for row in features:\n",
    "        csv_out.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to delete\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "#print X.shape, y.shape, training_features.shape\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "   training_features, y, test_size=0.01, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score:  1.0\n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "# to delete\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=15, n_jobs=4, max_features=8)\n",
    "clf.fit(X_train, y_train)\n",
    "y_val_pred = clf.predict(X_val)\n",
    "f1score = f1_score(y_val, y_val_pred)\n",
    "acc = accuracy_score(y_val, y_val_pred)\n",
    "print 'F1-score: ', f1score\n",
    "print 'Accuracy: ', acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test_pred = clf.predict(X_test)\n",
    "\n",
    "test_predictions_random = zip(range(len(y_test_pred)), y_test_pred)\n",
    "\n",
    "with open(\"unscaled_randomforest_authors.csv\",\"wb\") as pred1:\n",
    "    csv_out = csv.writer(pred1)\n",
    "    csv_out.writerow(('ID','category'))\n",
    "    for row in test_predictions_random:\n",
    "        csv_out.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "244px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
