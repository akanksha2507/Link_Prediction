{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random, csv\n",
    "import numpy as np\n",
    "import cPickle\n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn import preprocessing\n",
    "import nltk\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading test and train data, and node info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#nltk.download('punkt') # for tokenization\n",
    "#nltk.download('stopwords')\n",
    "stpwds = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "stemmer_new = nltk.stem.SnowballStemmer(\"english\")\n",
    "with open(\"testing_set.txt\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    testing_set  = list(reader)\n",
    "\n",
    "testing_set = [element[0].split(\" \") for element in testing_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"training_set.txt\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    training_set  = list(reader)\n",
    "\n",
    "training_set = [element[0].split(\" \") for element in training_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"node_information.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    node_info  = list(reader)\n",
    "\n",
    "IDs = [element[0] for element in node_info]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9407087 -0.411383 -0.020357 -0.400334 -0.339862 -0.135672 -0.253496 0.186777 -0.343075 0.006982 -0.133885 -0.226440 0.221926 -0.095856 0.026859 -0.541065 -0.032277 0.158725 -0.163885 -0.013826 0.255544 0.155507 -0.306157 0.159946 0.193400 -0.024856 0.263089 0.044932 -0.202525 -0.395658 0.246506 0.039258 0.118137 -0.087199 -0.158658 0.039355 0.102043 0.151044 -0.160539 -0.147107 -0.027377 0.116614 -0.097601 0.091300 -0.048201 0.139412 -0.223881 -0.001322 -0.385248 0.019185 0.042991 -0.054089 -0.078147 -0.397054 -0.093546 -0.327359 0.443369 0.268929 -0.053761 -0.198621 -0.074796 0.303797 -0.048937 0.078628 -0.357489 0.221393 -0.000290 -0.204928 -0.210304 -0.105123 0.081045 -0.004886 -0.089612 0.194011 -0.103109 -0.069539 -0.005624 -0.016682 0.108760 -0.089497 0.109126 -0.620871 0.299417 -0.029055 0.018739 -0.064403 0.246944 0.135511 -0.104967 -0.086793 -0.091969 0.025420 0.352654 0.065260 -0.134090 0.025244 0.412901 -0.147076 0.105749 0.247415 0.004214 0.017703 -0.205978 -0.215702 -0.122759 0.156313 0.370237 -0.040112 -0.214507 -0.021264 0.201224 -0.035294 -0.070247 0.006301 0.165471 -0.237350 0.247316 -0.640172 -0.129682 0.047887 0.502395 -0.137383 -0.131222 -0.035897 -0.027176 0.103022 -0.059969 0.419557 0.321013\n",
      "\n"
     ]
    }
   ],
   "source": [
    "featfile = open(\"embeddings_new_1000.emd\", 'r')\n",
    "featline = featfile.readlines()\n",
    "print featline[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27770L, 128L)\n"
     ]
    }
   ],
   "source": [
    "node_embeds = []\n",
    "node_nums = []\n",
    "for line in featline[1:]:\n",
    "    line = line.strip().split(\" \")\n",
    "    node_embeds.append(np.array(line[1:], dtype=float))\n",
    "    node_nums.append(line[0])\n",
    "nodes_vec = np.array(node_embeds)  \n",
    "print np.shape(nodes_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k = 20\n",
    "kmeans = KMeans(n_clusters=k, n_jobs=-1).fit(nodes_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_pred = kmeans.predict(nodes_vec).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_node_pred = dict(zip(node_nums, clusters_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 training examples processsed\n",
      "100001 training examples processsed\n",
      "200001 training examples processsed\n",
      "300001 training examples processsed\n",
      "400001 training examples processsed\n",
      "500001 training examples processsed\n",
      "600001 training examples processsed\n"
     ]
    }
   ],
   "source": [
    "file=open(\"features_train_unscaled.csv\",'r')\n",
    "lines=file.readlines()\n",
    "feat = []\n",
    "self_cit = []\n",
    "counter = 0\n",
    "for line in training_set:\n",
    "    #line=line.strip().split(\",\")  \n",
    "    #feat.append(line)\n",
    "    source = line[0]\n",
    "    target = line[1]\n",
    "    \n",
    "    feat.append([line[2], int(cluster_node_pred[source] == cluster_node_pred[target])])\n",
    "    counter += 1\n",
    "    if counter % 100000 == True:\n",
    "        print counter, \"training examples processsed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', 0]"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating features for the entire data to store in csv file\n",
    "##### since this is very slow, I am writing only 50000 examples for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(615512L, 1L) (615512L,)\n"
     ]
    }
   ],
   "source": [
    "## delete - just trying out this feature before writing into file\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "X = np.array(feat)[:, 3:]\n",
    "#y = train_features[:,2]\n",
    "y = np.array(feat)[:,2]\n",
    "y = np.array(y, dtype=int)\n",
    "print X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(412393L, 1L) (203119L, 1L) (412393L,) (203119L,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RandomForestClassifier()#, max_features=2)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "print X_train.shape, X_val.shape, y_train.shape, y_val.shape\n",
    "clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score:  0.714486049098\n",
      "Accuracy:  0.738384887677\n"
     ]
    }
   ],
   "source": [
    "y_val_pred = clf.predict(X_val)\n",
    "f1score = f1_score(y_val, y_val_pred)\n",
    "acc = accuracy_score(y_val, y_val_pred)\n",
    "print 'F1-score: ', f1score\n",
    "print 'Accuracy: ', acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k:  20 , F1-score:  0.713800205419 , Accuracy:  0.737976260222\n"
     ]
    }
   ],
   "source": [
    "# For running kmeans with different values of k and computing F1 score\n",
    "for k in [20]:\n",
    "    kmeans = KMeans(n_clusters=k, n_jobs=-1).fit(nodes_vec)\n",
    "    clusters_pred = kmeans.predict(nodes_vec).tolist()\n",
    "    cluster_node_pred = dict(zip(node_nums, clusters_pred))\n",
    "    counter = 0\n",
    "    for line in training_set:\n",
    "        source = line[0]\n",
    "        target = line[1]\n",
    "    \n",
    "        feat.append([line[2], int(cluster_node_pred[source] == cluster_node_pred[target])])\n",
    "        counter += 1\n",
    "        #if counter % 100000 == True:\n",
    "            #print counter, \"training examples processsed\"\n",
    "\n",
    "    X = np.array(feat)[:, 1:]\n",
    "    y = np.array(feat)[:,0]\n",
    "    y = np.array(y, dtype=int)\n",
    "\n",
    "    clf = RandomForestClassifier()#, max_features=2)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "    #print X_train.shape, X_val.shape, y_train.shape, y_val.shape\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_val_pred = clf.predict(X_val)\n",
    "    f1score = f1_score(y_val, y_val_pred)\n",
    "    acc = accuracy_score(y_val, y_val_pred)\n",
    "    print 'k: ', k, ', F1-score: ', f1score, ', Accuracy: ', acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "244px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
