{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random, csv\n",
    "import numpy as np\n",
    "import scipy\n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn import preprocessing\n",
    "import nltk\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading test and train data, and node info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#nltk.download('punkt') # for tokenization\n",
    "#nltk.download('stopwords')\n",
    "stpwds = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "\n",
    "with open(\"testing_set.txt\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    testing_set  = list(reader)\n",
    "\n",
    "testing_set = [element[0].split(\" \") for element in testing_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"training_set.txt\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    training_set  = list(reader)\n",
    "\n",
    "training_set = [element[0].split(\" \") for element in training_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"node_information.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    node_info  = list(reader)\n",
    "\n",
    "IDs = [element[0] for element in node_info]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating graph (nodes and edges from training data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "edges = [(element[0],element[1]) for element in training_set if element[2]==\"1\"]\n",
    "nodes = [element[0] for element in training_set] + [element[1] for element in training_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is the actual directed graph\n",
    "G = nx.DiGraph()\n",
    "G.add_nodes_from(nodes)\n",
    "G.add_edges_from(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# undirected version of the graph (used for jaccard similarity)\n",
    "G_undirected = nx.Graph()\n",
    "G_undirected.add_nodes_from(nodes)\n",
    "G_undirected.add_edges_from(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### KATZ!!!! - very slow\n",
    "\n",
    "#adj_mat = nx.to_scipy_sparse_matrix(G, format='csc')\n",
    "#identity = scipy.sparse.eye(adj_mat.shape[0])\n",
    "#temp = identity - (0.9)*adj_mat\n",
    "#katz_score = scipy.sparse.linalg.inv(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_EXAMPLES = 10000\n",
    "NUM_TRAIN = 7000\n",
    "NUM_VALIDATION = 3000\n",
    "NUM_FEATURES = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parameters to run on entire data\n",
    "# NUM_EXAMPLES = len(training_set)\n",
    "# NUM_TRAIN = int(0.7*NUM_EXAMPLES)\n",
    "# NUM_VALIDATION = int(0.3*NUM_EXAMPLES)\n",
    "# NUM_FEATURES = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading existing features for the entire data & add the new feature and store in csv file\n",
    "##### since this is very slow, I am writing only 50000 examples for now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading features from features.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['source', 'target', 'label', 'overlap_title', 'diff', 'common_author', 'jaccard_sim']\n"
     ]
    }
   ],
   "source": [
    "featuresFile = \"featuresNEW.csv\" if NUM_EXAMPLES == 10000 else \"features.csv\" if NUM_EXAMPLES == 100000 \\\n",
    "        else \"features_train_2.csv\"\n",
    "with open(featuresFile, \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    existing_features  = list(reader)\n",
    "print existing_features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "615513"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(existing_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 training examples processsed\n",
      "2001 training examples processsed\n",
      "4001 training examples processsed\n",
      "6001 training examples processsed\n",
      "8001 training examples processsed\n",
      "10001 training examples processsed\n",
      "12001 training examples processsed\n",
      "14001 training examples processsed\n",
      "16001 training examples processsed\n",
      "18001 training examples processsed\n",
      "20001 training examples processsed\n",
      "22001 training examples processsed\n",
      "24001 training examples processsed\n",
      "26001 training examples processsed\n",
      "28001 training examples processsed\n",
      "30001 training examples processsed\n",
      "32001 training examples processsed\n",
      "34001 training examples processsed\n",
      "36001 training examples processsed\n",
      "38001 training examples processsed\n",
      "40001 training examples processsed\n",
      "42001 training examples processsed\n",
      "44001 training examples processsed\n",
      "46001 training examples processsed\n",
      "48001 training examples processsed\n",
      "50001 training examples processsed\n",
      "52001 training examples processsed\n",
      "54001 training examples processsed\n",
      "56001 training examples processsed\n",
      "58001 training examples processsed\n",
      "60001 training examples processsed\n",
      "62001 training examples processsed\n",
      "64001 training examples processsed\n",
      "66001 training examples processsed\n",
      "68001 training examples processsed\n",
      "70001 training examples processsed\n",
      "72001 training examples processsed\n",
      "74001 training examples processsed\n",
      "76001 training examples processsed\n",
      "78001 training examples processsed\n",
      "80001 training examples processsed\n",
      "82001 training examples processsed\n",
      "84001 training examples processsed\n",
      "86001 training examples processsed\n",
      "88001 training examples processsed\n",
      "90001 training examples processsed\n",
      "92001 training examples processsed\n",
      "94001 training examples processsed\n",
      "96001 training examples processsed\n",
      "98001 training examples processsed\n",
      "100001 training examples processsed\n",
      "102001 training examples processsed\n",
      "104001 training examples processsed\n",
      "106001 training examples processsed\n",
      "108001 training examples processsed\n",
      "110001 training examples processsed\n",
      "112001 training examples processsed\n",
      "114001 training examples processsed\n",
      "116001 training examples processsed\n",
      "118001 training examples processsed\n",
      "120001 training examples processsed\n",
      "122001 training examples processsed\n",
      "124001 training examples processsed\n",
      "126001 training examples processsed\n",
      "128001 training examples processsed\n",
      "130001 training examples processsed\n",
      "132001 training examples processsed\n",
      "134001 training examples processsed\n",
      "136001 training examples processsed\n",
      "138001 training examples processsed\n",
      "140001 training examples processsed\n",
      "142001 training examples processsed\n",
      "144001 training examples processsed\n",
      "146001 training examples processsed\n",
      "148001 training examples processsed\n",
      "150001 training examples processsed\n",
      "152001 training examples processsed\n",
      "154001 training examples processsed\n",
      "156001 training examples processsed\n",
      "158001 training examples processsed\n",
      "160001 training examples processsed\n",
      "162001 training examples processsed\n",
      "164001 training examples processsed\n",
      "166001 training examples processsed\n",
      "168001 training examples processsed\n",
      "170001 training examples processsed\n",
      "172001 training examples processsed\n",
      "174001 training examples processsed\n",
      "176001 training examples processsed\n",
      "178001 training examples processsed\n",
      "180001 training examples processsed\n",
      "182001 training examples processsed\n",
      "184001 training examples processsed\n",
      "186001 training examples processsed\n",
      "188001 training examples processsed\n",
      "190001 training examples processsed\n",
      "192001 training examples processsed\n",
      "194001 training examples processsed\n",
      "196001 training examples processsed\n",
      "198001 training examples processsed\n",
      "200001 training examples processsed\n",
      "202001 training examples processsed\n",
      "204001 training examples processsed\n",
      "206001 training examples processsed\n",
      "208001 training examples processsed\n",
      "210001 training examples processsed\n",
      "212001 training examples processsed\n",
      "214001 training examples processsed\n",
      "216001 training examples processsed\n",
      "218001 training examples processsed\n",
      "220001 training examples processsed\n",
      "222001 training examples processsed\n",
      "224001 training examples processsed\n",
      "226001 training examples processsed\n",
      "228001 training examples processsed\n",
      "230001 training examples processsed\n",
      "232001 training examples processsed\n",
      "234001 training examples processsed\n",
      "236001 training examples processsed\n",
      "238001 training examples processsed\n",
      "240001 training examples processsed\n",
      "242001 training examples processsed\n",
      "244001 training examples processsed\n",
      "246001 training examples processsed\n",
      "248001 training examples processsed\n",
      "250001 training examples processsed\n",
      "252001 training examples processsed\n",
      "254001 training examples processsed\n",
      "256001 training examples processsed\n",
      "258001 training examples processsed\n",
      "260001 training examples processsed\n",
      "262001 training examples processsed\n",
      "264001 training examples processsed\n",
      "266001 training examples processsed\n",
      "268001 training examples processsed\n",
      "270001 training examples processsed\n",
      "272001 training examples processsed\n",
      "274001 training examples processsed\n",
      "276001 training examples processsed\n",
      "278001 training examples processsed\n",
      "280001 training examples processsed\n",
      "282001 training examples processsed\n",
      "284001 training examples processsed\n",
      "286001 training examples processsed\n",
      "288001 training examples processsed\n",
      "290001 training examples processsed\n",
      "292001 training examples processsed\n",
      "294001 training examples processsed\n",
      "296001 training examples processsed\n",
      "298001 training examples processsed\n",
      "300001 training examples processsed\n",
      "302001 training examples processsed\n",
      "304001 training examples processsed\n",
      "306001 training examples processsed\n",
      "308001 training examples processsed\n",
      "310001 training examples processsed\n",
      "312001 training examples processsed\n",
      "314001 training examples processsed\n",
      "316001 training examples processsed\n",
      "318001 training examples processsed\n",
      "320001 training examples processsed\n",
      "322001 training examples processsed\n",
      "324001 training examples processsed\n",
      "326001 training examples processsed\n",
      "328001 training examples processsed\n",
      "330001 training examples processsed\n",
      "332001 training examples processsed\n",
      "334001 training examples processsed\n",
      "336001 training examples processsed\n",
      "338001 training examples processsed\n",
      "340001 training examples processsed\n",
      "342001 training examples processsed\n",
      "344001 training examples processsed\n",
      "346001 training examples processsed\n",
      "348001 training examples processsed\n",
      "350001 training examples processsed\n",
      "352001 training examples processsed\n",
      "354001 training examples processsed\n",
      "356001 training examples processsed\n",
      "358001 training examples processsed\n",
      "360001 training examples processsed\n",
      "362001 training examples processsed\n",
      "364001 training examples processsed\n",
      "366001 training examples processsed\n",
      "368001 training examples processsed\n",
      "370001 training examples processsed\n",
      "372001 training examples processsed\n",
      "374001 training examples processsed\n",
      "376001 training examples processsed\n",
      "378001 training examples processsed\n",
      "380001 training examples processsed\n",
      "382001 training examples processsed\n",
      "384001 training examples processsed\n",
      "386001 training examples processsed\n",
      "388001 training examples processsed\n",
      "390001 training examples processsed\n",
      "392001 training examples processsed\n",
      "394001 training examples processsed\n",
      "396001 training examples processsed\n",
      "398001 training examples processsed\n",
      "400001 training examples processsed\n",
      "402001 training examples processsed\n",
      "404001 training examples processsed\n",
      "406001 training examples processsed\n",
      "408001 training examples processsed\n",
      "410001 training examples processsed\n",
      "412001 training examples processsed\n",
      "414001 training examples processsed\n",
      "416001 training examples processsed\n",
      "418001 training examples processsed\n",
      "420001 training examples processsed\n",
      "422001 training examples processsed\n",
      "424001 training examples processsed\n",
      "426001 training examples processsed\n",
      "428001 training examples processsed\n",
      "430001 training examples processsed\n",
      "432001 training examples processsed\n",
      "434001 training examples processsed\n",
      "436001 training examples processsed\n",
      "438001 training examples processsed\n",
      "440001 training examples processsed\n",
      "442001 training examples processsed\n",
      "444001 training examples processsed\n",
      "446001 training examples processsed\n",
      "448001 training examples processsed\n",
      "450001 training examples processsed\n",
      "452001 training examples processsed\n",
      "454001 training examples processsed\n",
      "456001 training examples processsed\n",
      "458001 training examples processsed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "460001 training examples processsed\n",
      "462001 training examples processsed\n",
      "464001 training examples processsed\n",
      "466001 training examples processsed\n",
      "468001 training examples processsed\n",
      "470001 training examples processsed\n",
      "472001 training examples processsed\n",
      "474001 training examples processsed\n",
      "476001 training examples processsed\n",
      "478001 training examples processsed\n",
      "480001 training examples processsed\n",
      "482001 training examples processsed\n",
      "484001 training examples processsed\n",
      "486001 training examples processsed\n",
      "488001 training examples processsed\n",
      "490001 training examples processsed\n",
      "492001 training examples processsed\n",
      "494001 training examples processsed\n",
      "496001 training examples processsed\n",
      "498001 training examples processsed\n",
      "500001 training examples processsed\n",
      "502001 training examples processsed\n",
      "504001 training examples processsed\n",
      "506001 training examples processsed\n",
      "508001 training examples processsed\n",
      "510001 training examples processsed\n",
      "512001 training examples processsed\n",
      "514001 training examples processsed\n",
      "516001 training examples processsed\n",
      "518001 training examples processsed\n",
      "520001 training examples processsed\n",
      "522001 training examples processsed\n",
      "524001 training examples processsed\n",
      "526001 training examples processsed\n",
      "528001 training examples processsed\n",
      "530001 training examples processsed\n",
      "532001 training examples processsed\n",
      "534001 training examples processsed\n",
      "536001 training examples processsed\n",
      "538001 training examples processsed\n",
      "540001 training examples processsed\n",
      "542001 training examples processsed\n",
      "544001 training examples processsed\n",
      "546001 training examples processsed\n",
      "548001 training examples processsed\n",
      "550001 training examples processsed\n",
      "552001 training examples processsed\n",
      "554001 training examples processsed\n",
      "556001 training examples processsed\n",
      "558001 training examples processsed\n",
      "560001 training examples processsed\n",
      "562001 training examples processsed\n",
      "564001 training examples processsed\n",
      "566001 training examples processsed\n",
      "568001 training examples processsed\n",
      "570001 training examples processsed\n",
      "572001 training examples processsed\n",
      "574001 training examples processsed\n",
      "576001 training examples processsed\n",
      "578001 training examples processsed\n",
      "580001 training examples processsed\n",
      "582001 training examples processsed\n",
      "584001 training examples processsed\n",
      "586001 training examples processsed\n",
      "588001 training examples processsed\n",
      "590001 training examples processsed\n",
      "592001 training examples processsed\n",
      "594001 training examples processsed\n",
      "596001 training examples processsed\n",
      "598001 training examples processsed\n",
      "600001 training examples processsed\n",
      "602001 training examples processsed\n",
      "604001 training examples processsed\n",
      "606001 training examples processsed\n",
      "608001 training examples processsed\n",
      "610001 training examples processsed\n",
      "612001 training examples processsed\n",
      "614001 training examples processsed\n"
     ]
    }
   ],
   "source": [
    "# create the next feature - ? measure\n",
    "resource_alloc, pre_at, common_neigh = [], [], []\n",
    "counter = 0\n",
    "\n",
    "for i in xrange(len(training_set[:NUM_EXAMPLES])):\n",
    "    source = training_set[i][0]\n",
    "    target = training_set[i][1]\n",
    "    \n",
    "    # calculate the measure\n",
    "    common_neigh.append(len(sorted(nx.common_neighbors(G_undirected, source, target))))\n",
    "\n",
    "    res = nx.preferential_attachment(G_undirected, [(source, target)])\n",
    "    for r in res:\n",
    "        pre_at.append(r[2])\n",
    "    \n",
    "    res = nx.resource_allocation_index(G_undirected, [(source, target)])\n",
    "    for r in res:\n",
    "        resource_alloc.append(r[2])\n",
    "    \n",
    "    counter += 1\n",
    "    if counter % 2000 == True:\n",
    "        print counter, \"training examples processsed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "common_neigh = preprocessing.scale(common_neigh)\n",
    "pre_at = preprocessing.scale(pre_at)\n",
    "resource_alloc = preprocessing.scale(resource_alloc)\n",
    "#print s_paths[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['source', 'target', 'label', 'overlap_title', 'diff', 'common_author', 'jaccard_sim', 'common_neigh', 'pre_at', 'resource_alloc']\n"
     ]
    }
   ],
   "source": [
    "feats = []\n",
    "feats.append(existing_features[0] + ['common_neigh'] + ['pre_at'] + ['resource_alloc'])\n",
    "for i in range(NUM_EXAMPLES):\n",
    "    feats.append(existing_features[i+1] + [common_neigh[i]] + [pre_at[i]] + [resource_alloc[i]])\n",
    "print feats[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing features to features.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write features to .csv file\n",
    "with open(featuresFile, \"wb\") as feat:\n",
    "    csv_out = csv.writer(feat)\n",
    "    for row in feats:\n",
    "        csv_out.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(615513,\n",
       " ['1.6332984567897153',\n",
       "  '-0.32844506272978868',\n",
       "  '-0.23343490079502244',\n",
       "  '0.0043584186047233879',\n",
       "  -0.46980830711435273,\n",
       "  -0.22321280779425962,\n",
       "  0.070963551867491301])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feats), feats[1][3:NUM_FEATURES+3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_TRAIN = NUM_EXAMPLES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating training_features and validation_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert labels into integers then into column array\n",
    "\n",
    "labels = [int(element[2]) for element in feats[1:NUM_TRAIN+1]]\n",
    "labels_array = np.array(labels)\n",
    "training_features = [element[3:NUM_FEATURES+3] for element in feats[1:NUM_TRAIN+1]]\n",
    "training_features = np.array(training_features, dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(615512,\n",
       " array([ 1.63329846, -0.32844506, -0.2334349 ,  0.00435842, -0.46980831,\n",
       "        -0.22321281,  0.07096355]))"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_features), training_features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert labels into integers then into column array\n",
    "labels_validation = [int(element[2]) for element in feats[NUM_TRAIN+1:]]\n",
    "#labels_validation = list(labels_validation)\n",
    "labels_array_validation = np.array(labels_validation)\n",
    "validation_features = [element[3:NUM_FEATURES+3] for element in feats[NUM_TRAIN+1:]]\n",
    "validation_features = np.array(validation_features, dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(184654,\n",
       " array([-0.57150124,  1.09132875,  2.5652013 , -0.42066353, -0.29023304,\n",
       "        -0.05322793, -0.00262718]))"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validation_features), validation_features[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing basic SVM and predict on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize basic SVM\n",
    "classifier = svm.LinearSVC()\n",
    "\n",
    "# train\n",
    "classifier.fit(training_features, labels_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest & predict on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=9, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=80, n_jobs=1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = sklearn.ensemble.RandomForestClassifier(n_estimators=80, max_depth=9)\n",
    "rf.fit(training_features, labels_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions_SVM = list(rf.predict(validation_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network size:  27770\n",
      "Number of training samples:  615512\n",
      "Number of validation samples:  184653\n",
      "F1-score:  0.969030361397\n",
      "Accuracy:  0.966911087764\n"
     ]
    }
   ],
   "source": [
    "import sklearn \n",
    "f1score = sklearn.metrics.f1_score(labels_array_validation, predictions_SVM)\n",
    "acc = sklearn.metrics.accuracy_score(labels_array_validation, predictions_SVM)\n",
    "\n",
    "print 'Network size: ', len(G.nodes())\n",
    "print 'Number of training samples: ', NUM_TRAIN\n",
    "print 'Number of validation samples: ', NUM_VALIDATION\n",
    "print 'F1-score: ', f1score\n",
    "print 'Accuracy: ', acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the test data from features_test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['source', 'target', 'overlap_title', 'diff', 'common_author', 'jaccard_sim', 's_path', 'common_neigh', 'pref_att', 'resource_alloc'] ['source', 'target', 'overlap_title', 'diff', 'common_author', 'jaccard_sim']\n"
     ]
    }
   ],
   "source": [
    "with open(\"features_test.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    existing_features_test  = list(reader)\n",
    "print existing_features_test[0], existing_features_test[0][:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the next feature for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 training examples processsed\n",
      "2001 training examples processsed\n",
      "4001 training examples processsed\n",
      "6001 training examples processsed\n",
      "8001 training examples processsed\n",
      "10001 training examples processsed\n",
      "12001 training examples processsed\n",
      "14001 training examples processsed\n",
      "16001 training examples processsed\n",
      "18001 training examples processsed\n",
      "20001 training examples processsed\n",
      "22001 training examples processsed\n",
      "24001 training examples processsed\n",
      "26001 training examples processsed\n",
      "28001 training examples processsed\n",
      "30001 training examples processsed\n",
      "32001 training examples processsed\n"
     ]
    }
   ],
   "source": [
    "# create the next feature\n",
    "counter = 0\n",
    "resource_alloc_test = []\n",
    "pre_at_test, common_neigh_test = [], []\n",
    "for i in xrange(len(testing_set)):\n",
    "    source = testing_set[i][0]\n",
    "    target = testing_set[i][1]    \n",
    "    \n",
    "    # calculate the measure\n",
    "    common_neigh_test.append(len(sorted(nx.common_neighbors(G_undirected, source, target))))\n",
    "\n",
    "    res = nx.preferential_attachment(G_undirected, [(source, target)])\n",
    "    for r in res:\n",
    "        pre_at_test.append(r[2])\n",
    "    \n",
    "    res = nx.resource_allocation_index(G_undirected, [(source, target)])\n",
    "    for r in res:\n",
    "        resource_alloc_test.append(r[2])\n",
    "    \n",
    "    counter += 1\n",
    "    if counter % 2000 == True:\n",
    "        print counter, \"training examples processsed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "resource_alloc_test = preprocessing.scale(resource_alloc_test)\n",
    "pre_at_test = preprocessing.scale(pre_at_test)\n",
    "common_neigh_test = preprocessing.scale(common_neigh_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['source', 'target', 'overlap_title', 'diff', 'common_author', 'jaccard_sim', 'common_neigh', 'pre_at', 'resource_alloc']\n"
     ]
    }
   ],
   "source": [
    "feats_test = []\n",
    "feats_test.append(existing_features_test[0][:6] + ['common_neigh'] + ['pre_at'] + ['resource_alloc'])\n",
    "for i in range(len(testing_set)):\n",
    "    feats_test.append(existing_features_test[i+1][:6] + [common_neigh_test[i]] + [pre_at_test[i]] + [resource_alloc_test[i]])\n",
    "print feats_test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing features for test data to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write features to .csv file\n",
    "with open(\"features_test.csv\", \"wb\") as feat:\n",
    "    csv_out = csv.writer(feat)\n",
    "    for row in feats_test:\n",
    "        csv_out.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert labels into integers then into column array\n",
    "\n",
    "testing_features = [element[2:NUM_FEATURES+2] for element in feats_test[1:]]\n",
    "testing_features = np.array(testing_features, dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32648 [-0.56631167 -0.31764894 -0.23392443 -0.6281111  -0.56276568 -0.18520986\n",
      " -0.51311621] 32648 ['9807076', '9807139']\n"
     ]
    }
   ],
   "source": [
    "print len(testing_features), testing_features[0], len(testing_set), testing_set[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the model for test data and write the link predictions in the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# issue predictions\n",
    "test_predictions_SVM = list(rf.predict(testing_features))\n",
    "\n",
    "# write predictions to .csv file suitable for Kaggle (just make sure to add the column names)\n",
    "test_predictions_SVM = zip(range(len(testing_set)), test_predictions_SVM)\n",
    "\n",
    "with open(\"improved_predictions_common_neigh_rf.csv\",\"wb\") as pred1:\n",
    "    csv_out = csv.writer(pred1)\n",
    "    csv_out.writerow(('ID','category'))\n",
    "    for row in test_predictions_SVM:\n",
    "        csv_out.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
