{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import random, csv\n",
    "import numpy as np\n",
    "import cPickle\n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn import preprocessing\n",
    "import nltk\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Reading test and train data, and node info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#nltk.download('punkt') # for tokenization\n",
    "#nltk.download('stopwords')\n",
    "stpwds = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "stemmer_new = nltk.stem.SnowballStemmer(\"english\")\n",
    "with open(\"testing_set.txt\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    testing_set  = list(reader)\n",
    "\n",
    "testing_set = [element[0].split(\" \") for element in testing_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open(\"training_set.txt\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    training_set  = list(reader)\n",
    "\n",
    "training_set = [element[0].split(\" \") for element in training_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open(\"node_information.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    node_info  = list(reader)\n",
    "\n",
    "IDs = [element[0] for element in node_info]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Creating graph (nodes and edges from training data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "edges = [(element[0],element[1]) for element in training_set if element[2]==\"1\"]\n",
    "nodes = [element[0] for element in training_set] + [element[1] for element in training_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# this is the actual directed graph\n",
    "G = nx.DiGraph()\n",
    "G.add_nodes_from(nodes)\n",
    "G.add_edges_from(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# undirected version of the graph (used for jaccard similarity)\n",
    "G_undirected = nx.Graph()\n",
    "G_undirected.add_nodes_from(nodes)\n",
    "G_undirected.add_edges_from(edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# NUM_EXAMPLES = 100000\n",
    "# NUM_TRAIN = 70000\n",
    "# NUM_VALIDATION = 30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'dict'>\n"
     ]
    }
   ],
   "source": [
    "tfidf_dict = cPickle.load(open('tfidf_features.p', 'rb'))\n",
    "print type(tfidf_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0182400016638\n",
      "('9312155', '9506142')\n"
     ]
    }
   ],
   "source": [
    "print tfidf_dict[tfidf_dict.keys()[0]]\n",
    "print tfidf_dict.keys()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Creating features for the entire data to store in csv file\n",
    "##### since this is very slow, I am writing only 50000 examples for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 training examples processsed\n",
      "20001 training examples processsed\n",
      "40001 training examples processsed\n",
      "60001 training examples processsed\n",
      "80001 training examples processsed\n",
      "100001 training examples processsed\n",
      "120001 training examples processsed\n",
      "140001 training examples processsed\n",
      "160001 training examples processsed\n",
      "180001 training examples processsed\n",
      "200001 training examples processsed\n",
      "220001 training examples processsed\n",
      "240001 training examples processsed\n",
      "260001 training examples processsed\n",
      "280001 training examples processsed\n",
      "300001 training examples processsed\n",
      "320001 training examples processsed\n",
      "340001 training examples processsed\n",
      "360001 training examples processsed\n",
      "380001 training examples processsed\n",
      "400001 training examples processsed\n",
      "420001 training examples processsed\n",
      "440001 training examples processsed\n",
      "460001 training examples processsed\n",
      "480001 training examples processsed\n",
      "500001 training examples processsed\n",
      "520001 training examples processsed\n",
      "540001 training examples processsed\n",
      "560001 training examples processsed\n",
      "580001 training examples processsed\n",
      "600001 training examples processsed\n"
     ]
    }
   ],
   "source": [
    "### creating features and store in a csv file\n",
    "\n",
    "# number of overlapping words in title\n",
    "overlap_title = []\n",
    "\n",
    "# number of overlapping words in summary\n",
    "overlap_summary = []\n",
    "\n",
    "# temporal distance between the papers\n",
    "temp_diff = []\n",
    "\n",
    "# number of common authors\n",
    "comm_auth = []\n",
    "\n",
    "# jaccard similarity\n",
    "jaccard_sim = []\n",
    "\n",
    "# adamic score\n",
    "adamic = []\n",
    "\n",
    "tfidf = []\n",
    "\n",
    "counter = 0\n",
    "for i in xrange(len(training_set)):\n",
    "    source = training_set[i][0]\n",
    "    target = training_set[i][1]\n",
    "    \n",
    "    source_info = [element for element in node_info if element[0]==source][0]\n",
    "    target_info = [element for element in node_info if element[0]==target][0]\n",
    "    \n",
    "    # Title\n",
    "\t# convert to lowercase and tokenize\n",
    "    source_title = source_info[2].lower().split(\" \")\n",
    "\t# remove stopwords\n",
    "    source_title = [token for token in source_title if token not in stpwds]\n",
    "    source_title = [stemmer.stem(token) for token in source_title]\n",
    "    \n",
    "    target_title = target_info[2].lower().split(\" \")\n",
    "    target_title = [token for token in target_title if token not in stpwds]\n",
    "    target_title = [stemmer.stem(token) for token in target_title]\n",
    "    \n",
    "    # Author\n",
    "    source_auth = source_info[3].split(\",\")\n",
    "    target_auth = target_info[3].split(\",\")\n",
    "    \n",
    "    overlap_title.append(len(set(source_title).intersection(set(target_title))))\n",
    "    temp_diff.append(int(source_info[1]) - int(target_info[1]))\n",
    "    comm_auth.append(len(set(source_auth).intersection(set(target_auth))))\n",
    "   \n",
    "    # Summary\n",
    "    # convert to lowercase and tokenize\n",
    "    source_summary = source_info[2].lower().split(\" \")\n",
    "    source_summary = [token for token in source_summary if token not in stpwds]\n",
    "    source_summary = [stemmer_new.stem(token) for token in source_summary]\n",
    "    \n",
    "    target_summary = target_info[2].lower().split(\" \")\n",
    "    target_summary = [token for token in target_summary if token not in stpwds]\n",
    "    target_summary = [stemmer_new.stem(token) for token in target_summary]\n",
    "    \n",
    "    overlap_summary.append(len(set(source_summary).intersection(set(target_summary))))\n",
    "    \n",
    "    # Jaccard\n",
    "    preds = nx.jaccard_coefficient(G_undirected, [(source, target)])\n",
    "    for _, _, p in preds:\n",
    "        jaccard_sim.append(p)\n",
    "        \n",
    "    # Adamic adar\n",
    "    preds = nx.adamic_adar_index(G_undirected, [(source, target)])\n",
    "    for _, _, p in preds:\n",
    "        adamic.append(p)\n",
    "    \n",
    "    key = (source, target)\n",
    "    tfidf.append(tfidf_dict[key])\n",
    "    \n",
    "    counter += 1\n",
    "    if counter % 20000 == True:\n",
    "        print counter, \"training examples processsed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "training_features = np.array([overlap_title, temp_diff, comm_auth, overlap_summary, jaccard_sim, adamic, tfidf]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# # convert list of lists into array\n",
    "# # documents as rows, unique words as columns (i.e., example as rows, features as columns)\n",
    "# training_features = np.array([overlap_title, temp_diff, comm_auth, jaccard_sim]).T\n",
    "\n",
    "# # scale\n",
    "# training_features = preprocessing.scale(training_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(615512, 7)\n"
     ]
    }
   ],
   "source": [
    "print training_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# write features to .csv file\n",
    "X_train = np.array(training_set[:]).T\n",
    "X_features = training_features.T\n",
    "features = zip(X_train[0], X_train[1], X_train[2], X_features[0], X_features[1], X_features[2], X_features[3], X_features[4], \\\n",
    "              X_features[5], X_features[6])\n",
    "with open(\"features_train_unscaled.csv\",\"wb\") as feat:\n",
    "    csv_out = csv.writer(feat)\n",
    "    csv_out.writerow(('source', 'target', 'label', 'overlap_title', 'temp_diff', 'common_author', 'overlap_summary', 'jaccard_sim'\\\n",
    "                     ,'adamic', 'tfidf_cosine_sim'))\n",
    "    for row in features:\n",
    "        csv_out.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Computing features for test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'dict'>\n"
     ]
    }
   ],
   "source": [
    "tfidf_dict_test = cPickle.load(open('tfidf_features_test.p', 'rb'))\n",
    "print type(tfidf_dict_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 testing examples processsed\n",
      "10001 testing examples processsed\n",
      "20001 testing examples processsed\n",
      "30001 testing examples processsed\n"
     ]
    }
   ],
   "source": [
    "### creating features and store in a csv file\n",
    "\n",
    "# number of overlapping words in title\n",
    "overlap_title = []\n",
    "\n",
    "# number of overlapping words in summary\n",
    "overlap_summary = []\n",
    "\n",
    "# temporal distance between the papers\n",
    "temp_diff = []\n",
    "\n",
    "# number of common authors\n",
    "comm_auth = []\n",
    "\n",
    "# jaccard similarity\n",
    "jaccard_sim = []\n",
    "\n",
    "# adamic score\n",
    "adamic = []\n",
    "\n",
    "tfidf = []\n",
    "\n",
    "counter = 0\n",
    "for i in xrange(len(testing_set)):\n",
    "    source = testing_set[i][0]\n",
    "    target = testing_set[i][1]\n",
    "    \n",
    "    source_info = [element for element in node_info if element[0]==source][0]\n",
    "    target_info = [element for element in node_info if element[0]==target][0]\n",
    "    \n",
    "    # Title\n",
    "\t# convert to lowercase and tokenize\n",
    "    source_title = source_info[2].lower().split(\" \")\n",
    "\t# remove stopwords\n",
    "    source_title = [token for token in source_title if token not in stpwds]\n",
    "    source_title = [stemmer.stem(token) for token in source_title]\n",
    "    \n",
    "    target_title = target_info[2].lower().split(\" \")\n",
    "    target_title = [token for token in target_title if token not in stpwds]\n",
    "    target_title = [stemmer.stem(token) for token in target_title]\n",
    "    \n",
    "    # Author\n",
    "    source_auth = source_info[3].split(\",\")\n",
    "    target_auth = target_info[3].split(\",\")\n",
    "    \n",
    "    overlap_title.append(len(set(source_title).intersection(set(target_title))))\n",
    "    temp_diff.append(int(source_info[1]) - int(target_info[1]))\n",
    "    comm_auth.append(len(set(source_auth).intersection(set(target_auth))))\n",
    "   \n",
    "    # Summary\n",
    "    # convert to lowercase and tokenize\n",
    "    source_summary = source_info[2].lower().split(\" \")\n",
    "    source_summary = [token for token in source_summary if token not in stpwds]\n",
    "    source_summary = [stemmer_new.stem(token) for token in source_summary]\n",
    "    \n",
    "    target_summary = target_info[2].lower().split(\" \")\n",
    "    target_summary = [token for token in target_summary if token not in stpwds]\n",
    "    target_summary = [stemmer_new.stem(token) for token in target_summary]\n",
    "    \n",
    "    overlap_summary.append(len(set(source_summary).intersection(set(target_summary))))\n",
    "    \n",
    "    # Jaccard\n",
    "    preds = nx.jaccard_coefficient(G_undirected, [(source, target)])\n",
    "    for _, _, p in preds:\n",
    "        jaccard_sim.append(p)\n",
    "        \n",
    "    # Adamic adar\n",
    "    preds = nx.adamic_adar_index(G_undirected, [(source, target)])\n",
    "    for _, _, p in preds:\n",
    "        adamic.append(p)\n",
    "    \n",
    "    key = (source, target)\n",
    "    tfidf.append(tfidf_dict_test[key])\n",
    "    \n",
    "    counter += 1\n",
    "    if counter % 10000 == True:\n",
    "        print counter, \"testing examples processsed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "testing_features = np.array([overlap_title, temp_diff, comm_auth, overlap_summary, jaccard_sim, adamic, tfidf]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32648, 7)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# write features to .csv file\n",
    "X_test = np.array(testing_set[:]).T\n",
    "X_test_features = testing_features.T\n",
    "features = zip(X_test[0], X_test[1], X_test_features[0], X_test_features[1], X_test_features[2], X_test_features[3], X_test_features[4], \\\n",
    "              X_test_features[5], X_test_features[6])\n",
    "with open(\"features_test_unscaled.csv\",\"wb\") as feat:\n",
    "    csv_out = csv.writer(feat)\n",
    "    csv_out.writerow(('source', 'target', 'overlap_title', 'temp_diff', 'common_author', 'overlap_summary', 'jaccard_sim'\\\n",
    "                     ,'adamic', 'tfidf_cosine_sim'))\n",
    "    for row in features:\n",
    "        csv_out.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "244px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
